from torch import nn
import torch
from Embedding_LSTM import LSTM_Embedder
from Calc_Xi_And_LogLikelihood import Calc_Xi_And_LogLikelihood
from ResFlow import Resflow_Multi_Cluster
from Joint_Prob_SL import Joint_Probability_Multi_Cluster as Joint_Probability_Multi_Cluster_SL
from Joint_Prob_SIR import Joint_Probability_Multi_Cluster as Joint_Probability_Multi_Cluster_SIR

class SGVB_SSM_TS_Clustering_Model_With_NF(nn.Module):
    """
    A variational Bayesian model for time series clustering using DPMM with Normalizing Flows in a State Space Model.
    This version implements Dirichlet Process Mixture Models (DPMM) for dynamic clustering.
    """
    def __init__(self, 
                 base_param_dict,
                 lstm_param_dict,
                 resflow_param_dict,
                 model_param_dict,
                 joint_prob_param_dict,
                 alpha=1.0,  # Concentration parameter for DPMM
                 GPU=False):
        """
        Initializes the clustering model with DPMM for dynamic clusters.

        Parameters:
            base_param_dict (dict): Basic configuration parameters.
            lstm_param_dict (dict): Parameters for the LSTM embedder.
            resflow_param_dict (dict): Parameters for the residual flow models.
            model_param_dict (dict): Model parameters.
            joint_prob_param_dict (dict): Parameters for calculating the joint probability.
            alpha (float): Concentration parameter for DPMM.
            GPU (bool): If True, utilize CUDA for operations.
        """
        super().__init__()
        self.alpha = alpha  # Concentration parameter for Dirichlet Process
        self.GPU = GPU
    
        self.lstm_embedder = LSTM_Embedder(base_param_dict=base_param_dict, 
                                           lstm_param_dict=lstm_param_dict, 
                                           GPU=self.GPU)
        
        self.calc_xi_and_loglikelihood = Calc_Xi_And_LogLikelihood()
        
        # Initialize components
        self.resflow_multi_cluster = Resflow_Multi_Cluster(
            base_param_dict=base_param_dict,
            resflow_param_dict=resflow_param_dict
        )

    def stick_breaking_process(self, z):
        """
        Implements stick-breaking process for DPMM cluster assignment.
        
        Parameters:
            z (torch.Tensor): Input tensor (e.g., latent variables or cluster features).
        Returns:
            torch.Tensor: Cluster weights generated by stick-breaking.
        """
        # Sample Beta distribution for stick lengths
        beta_dist = torch.distributions.Beta(1, self.alpha)
        betas = beta_dist.sample(z.shape[:-1]).to(z.device)
        
        # Stick-breaking formula
        remaining_stick = torch.cumprod(1 - betas, dim=-1)
        cluster_weights = betas * torch.cat([torch.ones_like(remaining_stick[:, :1]), remaining_stick[:, :-1]], dim=-1)
        
        return cluster_weights
    
    def forward(self, obs_data):
        """
        Processes the observed data through the model components, using dynamic clusters via DPMM.

        Parameters:
            obs_data (torch.Tensor): The observed data tensor.

        Returns:
            dict: A dictionary containing the processed embeddings, log likelihoods, and transformed data.
        """
        data_embeded = self.lstm_embedder(obs_data)
        fw_res = self.forward_flow(data_embeded, obs_data)
        return {"data_embeded": data_embeded,
                "xi_loglh": fw_res["xi_loglh"],
                "z_logdet": fw_res["z_logdet"],
                "joint_prob": fw_res["joint_prob"]}

    def forward_flow(self, data_embeded, obs_data):
        xi_loglh = self.calc_xi_and_loglikelihood(data_embeded)
        z_logdet = self.resflow_multi_cluster(xi_loglh["xi"])
        
        # Call stick-breaking process here for dynamic cluster weights
        cluster_weights = self.stick_breaking_process(z_logdet["z"])
        joint_prob = self.calc_joint_probabilities(cluster_weights, z_logdet["z"], obs_data)
        
        return {"xi_loglh": xi_loglh, "z_logdet": z_logdet, "joint_prob": joint_prob}

    def calc_joint_probabilities(self, cluster_weights, z, obs_data):
        """
        Calculates joint probabilities based on DPMM cluster assignments.
        
        Parameters:
            cluster_weights (torch.Tensor): Cluster weights calculated using the stick-breaking process.
            z (torch.Tensor): Transformed latent variable using normalizing flows.
            obs_data (torch.Tensor): Observed data.
        
        Returns:
            torch.Tensor: Joint probabilities for each cluster.
        """
        # Step 1: Calculate likelihood of observed data given latent variables (z)
        # Example: assuming a Gaussian likelihood
        likelihood = torch.exp(-0.5 * (obs_data - z).pow(2))  # Adjust based on your data
        
        # Step 2: Combine cluster weights with the likelihood for each cluster
        joint_prob = cluster_weights * likelihood
        
        # Step 3: Normalize joint probabilities over clusters
        joint_prob_sum = joint_prob.sum(dim=-1, keepdim=True)
        joint_prob_normalized = joint_prob / joint_prob_sum
        
        return joint_prob_normalized

    def calc_loss(self, obs_data, alpha=None):
        """
        Calculates the loss for the model including dynamic clustering with DPMM.

        Parameters:
            obs_data (torch.Tensor): The observed data tensor.
            alpha (float, optional): Coefficient for entropy penalty (not required for DPMM).

        Returns:
            torch.Tensor: The calculated loss.
        """
        fw_res = self.forward(obs_data)
        ELBO = -fw_res["xi_loglh"]["LogLikelihood"] - fw_res["z_logdet"]["log_det"] + fw_res["joint_prob"]
        
        # No fixed number of clusters, use dynamic ELBO computation
        ELBO_sample_mean = (fw_res["data_embeded"]["cluster"] * ELBO).sum(axis=1)
        loss = -1 * torch.mean(ELBO_sample_mean)
        
        return loss
